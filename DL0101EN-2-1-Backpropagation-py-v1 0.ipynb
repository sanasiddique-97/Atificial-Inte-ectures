{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "947a6ec452597a91eaa4fa6ece0a600859c0684a4192c56e37ddb58e207db692"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n\n# BackPropagation\n\nEstimated time needed: **30** mins\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Backpropagation is the key algorithm used for training neural networks, allowing them to learn from data. It is based on the gradient descent optimization technique and works by iteratively adjusting the weights and biases of the network to minimize the error between the predicted and actual outputs.\n In this lab, we will create a neural network to implement backpropagation for a XOR problem.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Objective for this notebook\n\n* Train a Neural Network to Solve the XOR Problem\n* Implement Backpropagation for Neural Network Training\n* Demonstrate the Use of Activation Functions\n* Understand the Learning Process Over Multiple Epochs\n* Demonstrate Weight and Bias Adjustments via Gradient Descent\n* Evaluate the Model's Performance After Training\n* Monitor and Analyze the Training Process\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<h2>Table of Contents</h2>\n\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n<font size = 4>\n1. <a href=\"#Import-Required-Libraries\">Import Required Libraries</a><br>\n2. <a href=\"#Initialize-Inputs\">Initialize Inputs</a><br>\n3. <a href=\"#Declare-the-network-parameters\">Declare the network parameters</a><br>\n4. <a href=\"#Define-the-weights\">Define the weights</a><br>  \n5. <a href=\"#Training-the-Neural-Network\">Training the Neural Network</a><br>  \n6. <a href=\"#Testing-the-Network\">Testing the Network</a><br>  \n7. <a href=\"#Plot-the-error\">Plot the error</a><br>  \n\n</font>\n</div>\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. \n# If you run this notebook on a different environment, e.g., your desktop, you may need to uncomment and install certain libraries.\n\n#!pip install numpy==1.26.4\n#!pip install matplotlib==3.5.2",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Import Required Libraries\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Importing the required library\nimport numpy as np\nimport matplotlib.pyplot as plt",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Initialize Inputs\nDefine the input and expected output for a XOR gate problem\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Defining inputs and expected output (XOR truth table)\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # 2x4 matrix, each column is a training example\nd = np.array([0, 1, 1, 0])  # Expected output for XOR",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Declare the network parameters\n\nDefine the network parameters such as\n1. number of input neurons\n2. hidden layer neurons\n3. output neurons\n4. learning rate\n5. number of epochs\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Network parameters\ninputSize = 2     # Number of input neurons (x1, x2)\nhiddenSize = 2    # Number of hidden neurons\noutputSize = 1    # Number of output neurons\nlr = 0.1          # Learning rate\nepochs = 180000   # Number of training epochs",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Define the weights\nDeclare the weights for the neurons. The initial weights are taken as random numbers which are then optimized by the backpropagation algorithm\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Initialize weights and biases randomly within the range [-1, 1]\nw1 = np.random.rand(hiddenSize, inputSize) * 2 - 1  # Weights from input to hidden layer\nb1 = np.random.rand(hiddenSize, 1) * 2 - 1         # Bias for hidden layer\nw2 = np.random.rand(outputSize, hiddenSize) * 2 - 1  # Weights from hidden to output layer\nb2 = np.random.rand(outputSize, 1) * 2 - 1         # Bias for output layer",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Training the Neural Network\n\nThe neural network works in 5 stages: \n1. Forward pass\n    * The input **X** is multiplied by the weights **w1** and passed through the first layer, followed by the application of the sigmoid or ReLU activation function. This gives the output for the hidden layer.\n    * The output of the hidden layer is then passed through the second set of weights **w2** to compute the final output. Again, a sigmoid activation function is used to generate the final output **a2**.\n    \n2. Error calculation\n    * The error is computed as the difference between the expected output **(d)** and the actual output **(a2)**.\n3. Backward pass\n    * **Output Layer**: The derivative of the sigmoid activation function is applied to the error, producing the gradient for the output layer **(da2)**. This is used to calculate how much the weights in the output layer need to be adjusted.\n    * **Hidden Layer**: The error is then propagated backward to the hidden layer. The gradient at the hidden layer **(da1)** is computed by taking the dot product of the transpose of the weights **(w2.T)** and the gradient from the output layer. The derivative of the activation function (sigmoid or ReLU) is used to adjust this error.\n4. Weights and bias updates\n    * After computing the **gradients (dz1, dz2)**, the **weights (w1, w2)** and **biases (b1, b2)** are updated using the **learning rate (lr)** and **the gradients**. The updates are done to minimize the error and improve the modelâ€™s predictions.\n5. Training:\n    * This entire process is repeated over many iterations **(epochs)**. During each epoch, the model adjusts its weights and biases to reduce the error. Over time, the network learns to approximate the XOR function.\nForward Pass:\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Training the network using backpropagation\nerror_list = []\nfor epoch in range(epochs):\n    # Forward pass\n    z1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\n    a1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\n    z2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\n    a2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n    # Error calculation and backpropagation\n    error = d - a2  # Difference between expected and actual output\n    da2 = error * (a2 * (1 - a2))  # Derivative for output layer\n    dz2 = da2  # Gradient for output layer\n\n    # Propagate error to hidden layer\n    da1 = np.dot(w2.T, dz2)  # Gradient for hidden layer\n    dz1 = da1 * (a1 * (1 - a1))  # Derivative for hidden layer\n\n    # Update weights and biases\n    w2 += lr * np.dot(dz2, a1.T)  # Update weights from hidden to output layer\n    b2 += lr * np.sum(dz2, axis=1, keepdims=True)  # Update bias for output layer\n\n    w1 += lr * np.dot(dz1, X.T)  # Update weights from input to hidden layer\n    b1 += lr * np.sum(dz1, axis=1, keepdims=True)  # Update bias for hidden layer\n    if (epoch+1)%10000 == 0:\n        print(\"Epoch: %d, Average error: %0.05f\"%(epoch, np.average(abs(error))))\n        error_list.append(np.average(abs(error)))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Testing the Network\nAfter training, you can now test the neural network to verify that it has learned the XOR function and outputs the correct values close to [0, 1, 1, 0]\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Testing the trained network\nz1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\na1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\nz2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\na2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n# Print results\nprint('Final output after training:', a2)\nprint('Ground truth', d)\nprint('Error after training:', error)\nprint('Average error: %0.05f'%np.average(abs(error)))\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Plot the error\nHere, we plot the error as a function of epochs. This shows how error changed over multiple iterations of forward and backward passes and how the network learnt over time\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Plot error\nplt.plot(error_list)\nplt.title('Error')\nplt.xlabel('Epochs')\nplt.ylabel('Error')\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Practice exercise 1\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Implement backpropagation for **AND problem** using similar input as used for XOR problem above and plot the error\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Write your code here\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Double-click <b>here</b> for the solution.\n\n<!-- Your answer is below:\n\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # 2x4 matrix, each column is a training example\nd = np.array([0, 0, 0, 1])  # Expected output for AND\n\n# Training the network using backpropagation\nerror_list = []\nfor epoch in range(epochs):\n    # Forward pass\n    z1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\n    a1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\n    z2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\n    a2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n    # Error calculation and backpropagation\n    error = d - a2  # Difference between expected and actual output\n    da2 = error * (a2 * (1 - a2))  # Derivative for output layer\n    dz2 = da2  # Gradient for output layer\n\n    # Propagate error to hidden layer\n    da1 = np.dot(w2.T, dz2)  # Gradient for hidden layer\n    dz1 = da1 * (a1 * (1 - a1))  # Derivative for hidden layer\n\n    # Update weights and biases\n    w2 += lr * np.dot(dz2, a1.T)  # Update weights from hidden to output layer\n    b2 += lr * np.sum(dz2, axis=1, keepdims=True)  # Update bias for output layer\n\n    w1 += lr * np.dot(dz1, X.T)  # Update weights from input to hidden layer\n    b1 += lr * np.sum(dz1, axis=1, keepdims=True)  # Update bias for hidden layer\n    if (epoch+1)%10000 == 0:\n        print(\"Epoch: %d, Average error: %0.05f\"%(epoch, np.average(abs(error))))\n        error_list.append(np.average(abs(error)))\n\n\n# Testing the trained network\nz1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\na1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\nz2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\na2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n# Print results\nprint('\\nFinal output after training:', a2)\nprint('Ground truth', d)\nprint('Error after training:', error)\nprint('Average error: %0.05f'%np.average(abs(error)))\n\n# Plot error\nplt.plot(error_list)\nplt.title('Error')\nplt.xlabel('Epochs')\nplt.ylabel('Error')\nplt.show()\n\n\n-->\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Practice exercise 2\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, decrease the learning rate to 0.01 and increase the number of epochs to 1000000 and check the error for XOR gate\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Write your code here\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Double-click <b>here</b> for the solution.\n\n<!-- Your answer is below:\n\nlr = 0.01         # Learning rate\nepochs = 1000000   # Number of training epochs\n\n\n# Defining inputs and expected output (XOR truth table)\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T  # 2x4 matrix, each column is a training example\nd = np.array([0, 1, 1, 0])  # Expected output for XOR\n\n\n# Training the network using backpropagation\nerror_list = []\nfor epoch in range(epochs):\n    # Forward pass\n    z1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\n    a1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\n    z2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\n    a2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n    # Error calculation and backpropagation\n    error = d - a2  # Difference between expected and actual output\n    da2 = error * (a2 * (1 - a2))  # Derivative for output layer\n    dz2 = da2  # Gradient for output layer\n\n    # Propagate error to hidden layer\n    da1 = np.dot(w2.T, dz2)  # Gradient for hidden layer\n    dz1 = da1 * (a1 * (1 - a1))  # Derivative for hidden layer\n\n    # Update weights and biases\n    w2 += lr * np.dot(dz2, a1.T)  # Update weights from hidden to output layer\n    b2 += lr * np.sum(dz2, axis=1, keepdims=True)  # Update bias for output layer\n\n    w1 += lr * np.dot(dz1, X.T)  # Update weights from input to hidden layer\n    b1 += lr * np.sum(dz1, axis=1, keepdims=True)  # Update bias for hidden layer\n    if (epoch+1)%10000 == 0:\n        print(\"Epoch: %d, Average error: %0.05f\"%(epoch, np.average(abs(error))))\n        error_list.append(np.average(abs(error)))\n\n\n# Testing the trained network\nz1 = np.dot(w1, X) + b1  # Weighted sum for hidden layer\na1 = 1 / (1 + np.exp(-z1))  # Sigmoid activation for hidden layer\n\nz2 = np.dot(w2, a1) + b2  # Weighted sum for output layer\na2 = 1 / (1 + np.exp(-z2))  # Sigmoid activation for output layer\n\n# Print results\nprint('\\nFinal output after training:', a2)\nprint('Ground truth', d)\nprint('Error after training:', error)\nprint('Average error: %0.05f'%np.average(abs(error)))\n\n\n# Plot error\nplt.plot(error_list)\nplt.title('Error')\nplt.xlabel('Epochs')\nplt.ylabel('Error')\nplt.show()\n\n\n-->\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Thank you for completing this lab!\n\nThis notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n<!--\n## Change Log\n\n|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n|---|---|---|---|\n| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n<hr>\n\n## <h3 align=\"center\"> Â© IBM Corporation. All rights reserved. <h3/>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n",
      "metadata": {}
    }
  ]
}